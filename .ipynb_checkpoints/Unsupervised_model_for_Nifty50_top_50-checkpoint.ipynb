{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "479a55ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Symbol'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3790\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3789\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 3790\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[0;32m   3791\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[1;32mindex.pyx:152\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mindex.pyx:181\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7080\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:7088\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Symbol'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 26\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# statsmodels.regression.rolling: Importing the RollingOLS class for rolling linear regression.\u001b[39;00m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;66;03m# pandas_datareader.data: Importing the web module from pandas_datareader for fetching financial data.\u001b[39;00m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# matplotlib.pyplot: Importing the pyplot module for plotting.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     23\u001b[0m \n\u001b[0;32m     24\u001b[0m \u001b[38;5;66;03m# downloading the data\u001b[39;00m\n\u001b[0;32m     25\u001b[0m nifty50 \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_html(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://en.wikipedia.org/wiki/NIFTY_500\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m---> 26\u001b[0m symbols_list \u001b[38;5;241m=\u001b[39m nifty50[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSymbol\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique()\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Add '.NS' to each ticker symbol in the list\u001b[39;00m\n\u001b[0;32m     29\u001b[0m symbols_list \u001b[38;5;241m=\u001b[39m [ticker \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.NS\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m ticker \u001b[38;5;129;01min\u001b[39;00m symbols_list]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\frame.py:3896\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3894\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m   3895\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[1;32m-> 3896\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(key)\n\u001b[0;32m   3897\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[0;32m   3898\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3797\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   3792\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m   3793\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[0;32m   3794\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[0;32m   3795\u001b[0m     ):\n\u001b[0;32m   3796\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[1;32m-> 3797\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m   3798\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m   3799\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[0;32m   3800\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[0;32m   3801\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[0;32m   3802\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Symbol'"
     ]
    }
   ],
   "source": [
    "from statsmodels.regression.rolling import RollingOLS\n",
    "import pandas_datareader.data as web\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import datetime as dt\n",
    "import yfinance as yf\n",
    "import pandas_ta\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')  \n",
    "\n",
    "# statsmodels.regression.rolling: Importing the RollingOLS class for rolling linear regression.\n",
    "# pandas_datareader.data: Importing the web module from pandas_datareader for fetching financial data.\n",
    "# matplotlib.pyplot: Importing the pyplot module for plotting.\n",
    "# statsmodels.api: Importing the statsmodels library for statistical models and tests.\n",
    "# pandas: Importing the pandas library for data manipulation and analysis.\n",
    "# numpy: Importing the numpy library for numerical operations.\n",
    "# datetime: Importing the datetime module for working with dates.\n",
    "# yfinance: Importing the yfinance library for fetching financial data from Yahoo Finance.\n",
    "# pandas_ta: Importing the pandas_ta library for technical analysis.\n",
    "# warnings: Importing the warnings module to suppress warnings.\n",
    "\n",
    "# downloading the data\n",
    "nifty50 = pd.read_html('https://en.wikipedia.org/wiki/NIFTY_50')[2]\n",
    "symbols_list = nifty50['Symbol'].unique().tolist()\n",
    "\n",
    "# Add '.NS' to each ticker symbol in the list\n",
    "symbols_list = [ticker + '.NS' for ticker in symbols_list]\n",
    "\n",
    "# Display the modified list\n",
    "print(symbols_list)\n",
    "\n",
    "\n",
    "end_date = '2024-1-31'\n",
    "\n",
    "start_date = pd.to_datetime(end_date)-pd.DateOffset(365*8)\n",
    "\n",
    "# Setting start and end dates:\n",
    "# Defining the end_date as '2023-12-03'.\n",
    "# Calculating the start_date as 8 years (365 days * 8) before the end_date.capitalize\n",
    "\n",
    "df = yf.download(tickers=symbols_list,\n",
    "                 start=start_date,\n",
    "                 end=end_date).stack()\n",
    "\n",
    "df.index.names = ['date', 'ticker']\n",
    "\n",
    "df.columns = df.columns.str.lower()\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf44ede",
   "metadata": {},
   "source": [
    "### Calculate features and technical indicators\n",
    "\n",
    "* Garman -klass volatilit\n",
    "* RSI\n",
    "* Bollinger Bands\n",
    "* ATR\n",
    "* MACD\n",
    "* Dollar Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd079f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['garman_klass_vol'] = ((np.log(df['high'])-np.log(df['low']))**2)/2-(2*np.log(2)-1)*((np.log(df['adj close'])-np.log(df['open']))**2)\n",
    "\n",
    "# df: This is the DataFrame on which the grouping operation is applied.\n",
    "\n",
    "# .groupby(level=1): This is a method provided by pandas for grouping data based on one or more levels of a multi-level index. In this case, it's specifying to group the DataFrame by the second level of its index.\n",
    "\n",
    "# The index of df has two levels ('date' and 'ticker'). By specifying level=1, the grouping is done based on the 'ticker' level. This means that the DataFrame will be split into groups, each corresponding to a unique 'ticker' in the second level of the index.\n",
    "\n",
    "df['rsi'] = df.groupby(level=1)['adj close'].transform(lambda x: pandas_ta.rsi(close=x, length=20))\n",
    "\n",
    "df['bb_low'] = df.groupby(level=1)['adj close'].transform(lambda x: pandas_ta.bbands(close=np.log1p(x), length=20).iloc[:,0])\n",
    "                                                          \n",
    "df['bb_mid'] = df.groupby(level=1)['adj close'].transform(lambda x: pandas_ta.bbands(close=np.log1p(x), length=20).iloc[:,1])\n",
    "                                                          \n",
    "df['bb_high'] = df.groupby(level=1)['adj close'].transform(lambda x: pandas_ta.bbands(close=np.log1p(x), length=20).iloc[:,2])\n",
    "\n",
    "def compute_atr(stock_data):\n",
    "    atr = pandas_ta.atr(high=stock_data['high'],\n",
    "                        low=stock_data['low'],\n",
    "                        close=stock_data['close'],\n",
    "                        length=14)\n",
    "    return atr.sub(atr.mean()).div(atr.std())\n",
    "\n",
    "df['atr'] = df.groupby(level=1, group_keys=False).apply(compute_atr)\n",
    "\n",
    "def compute_macd(close):\n",
    "    macd = pandas_ta.macd(close=close, length=20).iloc[:,0]\n",
    "    return macd.sub(macd.mean()).div(macd.std())\n",
    "\n",
    "df['macd'] = df.groupby(level=1, group_keys=False)['adj close'].apply(compute_macd)\n",
    "\n",
    "df['dollar_volume'] = (df['adj close']*df['volume'])/1e6\n",
    "\n",
    "# 1e6 is a shorthand notation in Python for expressing the number 1 followed by 6 zeros, which is equivalent to 1,000,000. It is often used to represent one million in a more concise form.\n",
    "\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20f8cfc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of tables found: 3\n",
      "Table 0 columns:\n",
      "Index(['Unnamed: 0', 'Unnamed: 1'], dtype='object')\n",
      "==================================================\n",
      "Table 1 columns:\n",
      "Index(['Category', 'All-time highs[5]', 'All-time highs[5].1'], dtype='object')\n",
      "==================================================\n",
      "Table 2 columns:\n",
      "Index(['Sl.No', 'Company Name', 'Industry', 'Symbol', 'Series', 'ISIN Code'], dtype='object')\n",
      "==================================================\n",
      "Error: List index out of range.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read HTML tables from the Wikipedia page\n",
    "tables = pd.read_html('https://en.wikipedia.org/wiki/NIFTY_500#Other_Notable_Indices', header=0)\n",
    "\n",
    "# Print the number of tables found\n",
    "print(f\"Number of tables found: {len(tables)}\")\n",
    "\n",
    "# Print the columns of each table to understand their structure\n",
    "for i, table in enumerate(tables):\n",
    "    print(f\"Table {i} columns:\\n{table.columns}\\n{'='*50}\")\n",
    "\n",
    "# Check if the list has an element at the expected index\n",
    "if len(tables) > 4:\n",
    "    nifty50 = tables[4]\n",
    "    # Ensure the 'Symbol' column is present in the DataFrame\n",
    "    if 'Symbol' in nifty50.columns:\n",
    "        symbols_list = nifty50['Symbol'].unique().tolist()\n",
    "        # Add '.NS' to each ticker symbol in the list\n",
    "        symbols_list = [ticker + '.NS' for ticker in symbols_list]\n",
    "        print(\"Symbols List:\", symbols_list)\n",
    "    else:\n",
    "        print(\"Error: 'Symbol' column not found in the DataFrame.\")\n",
    "else:\n",
    "    print(\"Error: List index out of range.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26c662b0",
   "metadata": {},
   "source": [
    "# Taking First 30 Companies beacuse also listed in Sensex"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60ae631",
   "metadata": {},
   "source": [
    "## Getting monthly index and average dollar volume and making it multiindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10617a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "last_cols =[c for c in df.columns.unique(0)if c not in['dollar_volume','volume','open','high','low','close']]\n",
    "# exclude given column and include all the remaining one and store it in last_cols\n",
    "last_cols\n",
    "data=pd.concat([df.unstack('ticker')['dollar_volume'].resample('M').mean().stack('ticker').to_frame('dollar_volume'),\n",
    "# Resamples the 'dollar_volume' column by month, calculates the mean for each month, and then stacks the DataFrame.\n",
    "#The result is a DataFrame with a MultiIndex containing levels 'date' and 'ticker', with the 'dollar_volume' as a single column.\n",
    "# The capital M stands for month dollar volume mean is taken             \n",
    "df.unstack()[last_cols].resample('M').last().stack('ticker')],axis=1).dropna()\n",
    "# Unstacks the entire DataFrame and selects only the columns specified in last_cols.\n",
    "# Resamples the resulting DataFrame by month and selects the last value for each month.\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28499c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['dollar_volume'].unstack('ticker').rolling(5*12).mean()\n",
    "#This applies a rolling window operation with a window size of 5*12 (60) periods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53c2819a",
   "metadata": {},
   "source": [
    "# make changes here to adjust ticker name in dollar volume "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68a77162",
   "metadata": {},
   "outputs": [],
   "source": [
    "data['dollar_vol_rank']= (data.groupby('date')['dollar_volume'].rank(ascending=False))\n",
    "#selecting rows where the 'dollar_vol_rank' is less than 30.\n",
    "data = data[data['dollar_vol_rank']<500].drop(['dollar_volume','dollar_vol_rank'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e5082d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222e5e48",
   "metadata": {},
   "source": [
    "# Calculate Monthly returns for different time horizion features\n",
    "* TO capture time series dynamics that reflects ,for example,momentum patterns, we compute historical returns using the method .pct_change(lag) that is, returns over various monthly periods as identified by lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f09b6045",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_returns(df):\n",
    "    \n",
    "   \n",
    "    outlier_cutoff =0.005\n",
    "    # what is a outlier\n",
    "    # refer this video 'https://youtu.be/rZJbj2I-_Ek?feature=shared'\n",
    "    # an outlier helps us to konw irrelevance in data like 9,10,10,11,12,36 so outlier is used and 36 is removed\n",
    "    lags=[1,2,3,6,9,12]\n",
    "    for lag in lags:\n",
    "        df[f'return_{lag}m']=(df['adj close'] #: Creates a new column in the DataFrame for each lag, representing returns over that lag period.\n",
    "                            .pct_change(lag)  #Calculates the percentage change over the specified lag period.\n",
    "                            .pipe(lambda x: x.clip(lower=x.quantile(outlier_cutoff),upper=x.quantile(1-outlier_cutoff)))\n",
    "                            .add(1)\n",
    "                            .pow(1/lag)\n",
    "                            .sub(1))\n",
    "    return df\n",
    "\n",
    "data = data.groupby(level=1, group_keys=False).apply(calculate_returns).dropna() #Applies the calculate_returns function to each group of the DataFrame, grouping by the second level of the index ('ticker').\n",
    "# .dropna(): Drops any rows with missing values in the resulting DataFrame.\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c90988e5",
   "metadata": {},
   "source": [
    "## Download Fama-French Factors and Calculate Rolling Factor Betas.\n",
    "* We will introduce the Fama—French data to estimate the exposure of assets to common risk factors using linear regression.\n",
    "\n",
    "* The five Fama—French factors, namely market risk, size, value, operating profitability, and investment have been shown empirically to explain asset returns and are commonly used to assess the risk/return profile of portfolios. Hence, it is natural to include past factor exposures as financial features in models.\n",
    "\n",
    "* We can access the historical factor returns using the pandas-datareader and estimate historical exposures using the RollingOLS rolling linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e10f12",
   "metadata": {},
   "source": [
    "### what is fama FRench model?\n",
    "\n",
    "* but Gene Fama and Kenneth French noticed that small-cap stocks (companies with smaller market capitalizations) tended to outperform large-cap stocks and that companies with a higher book-to-market ratio (value stocks) tended to outperform companies with a lower book-to-market ratio (growth stocks).\n",
    "\n",
    "* MKt-rf= it tells about market risk\n",
    "* SMB long on portfolio with small cap stocks and short on portfolio on the large cap stocks\n",
    "* HML long on portfolio with high book to market ratio and short on portfolio with low book to market ratio (low book to market ratio= companies that will grow eg paytm,ideaforage) high book to market are the value stocks\n",
    "* Robust minus weak (RMW), which compares the returns of firms with high, or robust, operating profitability\n",
    "* and those with weak, or low, operating profitability; and conservative minus aggressive (CMA)\n",
    "* we are comparing the number to usa stock market because no data has been available for indian stock market but the numbers are taken on yearly basis so error is less and numbers remain constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e7cc407",
   "metadata": {},
   "outputs": [],
   "source": [
    " factor_data = web.DataReader('F-F_Research_Data_5_Factors_2x3',\n",
    "                               'famafrench',\n",
    "                               start='2010')[0].drop('RF',axis=1)\n",
    "factor_data.index = factor_data.index.to_timestamp()\n",
    "factor_data= factor_data.resample('M').last().div(100)\n",
    "factor_data.index.name = 'date'\n",
    "factor_data = factor_data.join(data['return_1m']).sort_index()\n",
    "factor_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5d99153",
   "metadata": {},
   "outputs": [],
   "source": [
    "observation = factor_data.groupby(level=1).size()\n",
    "valid_stocks = observation[observation>=10]\n",
    "factor_data = factor_data[factor_data.index.get_level_values('ticker').isin(valid_stocks.index)]\n",
    "factor_data\n",
    "# Checks if each 'ticker' is in the index of valid stocks obtained in the previous step.\n",
    "# factor_data[factor_data.index.get_level_values('ticker').isin(valid_stocks.index)]: Filters the DataFrame to include only rows corresponding to valid stocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "326f7cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "betas =(factor_data.groupby(level=1,\n",
    "                            group_keys=False)\n",
    "         .apply(lambda x: RollingOLS(endog=x['return_1m'], \n",
    "                                     exog=sm.add_constant(x.drop('return_1m', axis=1)),\n",
    "                                     window=min(24, x.shape[0]),\n",
    "                                     min_nobs=len(x.columns)+1)\n",
    "         .fit(params_only=True)\n",
    "         .params\n",
    "         .drop('const', axis=1)))\n",
    "betas\n",
    "#the code calculates rolling betas for each stock by applying the RollingOLS model to the 'return_1m' as the dependent variable and the other factors from the factor_data DataFrame as independent variables. The rolling window size is \n",
    "# determined by the minimum of 24 and the number of observations available for each stock. The resulting betas DataFrame contains the time-varying beta coefficients for each stock with respect to the specified factors.\n",
    "# rolling returns gives the average of any stocks that give return for eg market is up 3% than it wont affect rolling return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ed02e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "factors=['Mkt-RF','SMB','HML','RMW','CMA']\n",
    "\n",
    "data =data.join(betas.groupby('ticker').shift())\n",
    "data.loc[:,factors] = data.groupby('ticker',group_keys=False)[factors].apply(lambda x: x.fillna(x.mean()))\n",
    "data = data.dropna()\n",
    "data = data.drop('adj close', axis=1)\n",
    "data\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7963ebd2",
   "metadata": {},
   "source": [
    "# At this point we have to decide on what ML model and approach to use for predictions etc.\n",
    "6. For each month fit a K-Means Clustering Algorithm to group similar assets based on their features.\n",
    "* K-Means Clustering\n",
    "\n",
    "* You may want to initialize predefined centroids for each cluster based on your research.\n",
    "\n",
    "* Then we will pre-define our centroids for each cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc097ac0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "#data = data.drop('cluster',axis=1) # added later after clustering with random\n",
    "\n",
    "def get_clusters(df):\n",
    "    df['cluster'] = KMeans(n_clusters=10,\n",
    "                           random_state=0,\n",
    "                           init='random').fit(df).labels_ # changed 'random' to initial centriods\n",
    "    return df\n",
    "data = data.dropna().groupby('date', group_keys=False).apply(get_clusters)\n",
    "data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15ac382a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_clusters(data):\n",
    "\n",
    "#     cluster_0 = data[data['cluster']==0]\n",
    "#     cluster_1 = data[data['cluster']==1]\n",
    "#     cluster_2 = data[data['cluster']==2]\n",
    "#     cluster_3 = data[data['cluster']==3]\n",
    "#     cluster_4 = data[data['cluster']==3]\n",
    "#     cluster_3 = data[data['cluster']==3]\n",
    "#     cluster_3 = data[data['cluster']==3]\n",
    "#     cluster_3 = data[data['cluster']==3]\n",
    "#     cluster_3 = data[data['cluster']==3]\n",
    "#     cluster_3 = data[data['cluster']==3]\n",
    "#     cluster_3 = data[data['cluster']==3]\n",
    "#     cluster_3 = data[data['cluster']==3]\n",
    "# # the impoertant thing we are chosing which column iloc[:,5] is atr and 1 is rsi\n",
    "#     plt.scatter(cluster_0.iloc[:,5] , cluster_0.iloc[:,1] , color = 'red', label='cluster 0')\n",
    "#     plt.scatter(cluster_1.iloc[:,5] , cluster_1.iloc[:,1] , color = 'green', label='cluster 1')\n",
    "#     plt.scatter(cluster_2.iloc[:,5] , cluster_2.iloc[:,1] , color = 'blue', label='cluster 2')\n",
    "#     plt.scatter(cluster_3.iloc[:,5] , cluster_3.iloc[:,1] , color = 'black', label='cluster 3')\n",
    "# # labeling\n",
    "#     x_label = data.columns[5]\n",
    "#     y_label = data.columns[1]\n",
    "#     plt.title(f'Clusters based on {x_label} and {y_label}')\n",
    "#     plt.xlabel(x_label)\n",
    "#     plt.ylabel(y_label)\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "#     return\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_clusters(data, num_clusters=10):\n",
    "    # Iterate over clusters\n",
    "    for cluster_num in range(num_clusters):\n",
    "        cluster_data = data[data['cluster'] ==cluster_num]\n",
    "        # Scatter plot for each cluster\n",
    "        plt.scatter(cluster_data.iloc[:, 5], cluster_data.iloc[:, 1], label=f'cluster {cluster_num}')\n",
    "\n",
    "    # Labeling\n",
    "    x_label = data.columns[5]\n",
    "    y_label = data.columns[1]\n",
    "   \n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage with 4 clusters\n",
    "# Replace data and 'cluster' with your actual DataFrame and cluster column name\n",
    "# plot_clusters(your_data_frame, num_clusters=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d3f5a36",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "\n",
    "for i in data.index.get_level_values('date').unique().tolist():\n",
    "    \n",
    "    g = data.xs(i, level=0)\n",
    "    \n",
    "    plt.title(f'Date {i}')\n",
    "    \n",
    "    plot_clusters(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e410b4",
   "metadata": {},
   "source": [
    "##  Apply pre-defined Centriods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a356eb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target_rsi_values = [30,35,40,45,55,60,65,70,75,80]\n",
    "\n",
    "initial_centroids = np.zeros((len(target_rsi_values), 18))\n",
    "\n",
    "initial_centroids[:, 1] = target_rsi_values\n",
    "\n",
    "initial_centroids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240a8ddf",
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\n",
    "data = data.drop('cluster', axis=1)\n",
    "\n",
    "def get_clusters(df):\n",
    "    df['cluster'] = KMeans(n_clusters=10,\n",
    "                           random_state=0,\n",
    "                           init=initial_centroids).fit(df).labels_\n",
    "    return df\n",
    "\n",
    "data = data.dropna().groupby('date', group_keys=False).apply(get_clusters)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf0add32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_clusters(data):\n",
    "\n",
    "#     cluster_0 = data[data['cluster']==0]\n",
    "#     cluster_1 = data[data['cluster']==1]\n",
    "#     cluster_2 = data[data['cluster']==2]\n",
    "#     cluster_3 = data[data['cluster']==3]\n",
    "\n",
    "#     plt.scatter(cluster_0.iloc[:,5] , cluster_0.iloc[:,1] , color = 'red', label='cluster 0')\n",
    "#     plt.scatter(cluster_1.iloc[:,5] , cluster_1.iloc[:,1] , color = 'green', label='cluster 1')\n",
    "#     plt.scatter(cluster_2.iloc[:,5] , cluster_2.iloc[:,1] , color = 'blue', label='cluster 2')\n",
    "#     plt.scatter(cluster_3.iloc[:,5] , cluster_3.iloc[:,1] , color = 'black', label='cluster 3')\n",
    "\n",
    "#     plt.legend()\n",
    "#     plt.show()\n",
    "#     return\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_clusters(data, num_clusters=10):\n",
    "    # Iterate over clusters\n",
    "    for cluster_num in range(num_clusters):\n",
    "        cluster_data = data[data['cluster'] == cluster_num]\n",
    "        # Scatter plot for each cluster\n",
    "        plt.scatter(cluster_data.iloc[:, 5], cluster_data.iloc[:, 1], label=f'cluster {cluster_num}')\n",
    "\n",
    "    # Labeling\n",
    "    x_label = data.columns[5]\n",
    "    y_label = data.columns[1]\n",
    "    plt.title(f'Clusters based on {x_label} and {y_label}')\n",
    "    plt.xlabel(x_label)\n",
    "    plt.ylabel(y_label)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Example usage with 4 clusters\n",
    "# Replace data and 'cluster' with your actual DataFrame and cluster column name\n",
    "# plot_clusters(your_data_frame, num_clusters=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "866dd867",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('ggplot')\n",
    "\n",
    "for i in data.index.get_level_values('date').unique().tolist():\n",
    "    \n",
    "    g = data.xs(i, level=0)\n",
    "    \n",
    "    plt.title(f'Date {i}')\n",
    "    \n",
    "    plot_clusters(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dec852",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_rsi_values = [30,35,40,45,55,60,65,70,75,80]\n",
    "\n",
    "initial_centroids = np.zeros((len(target_rsi_values), 18))\n",
    "\n",
    "initial_centroids[:, 1] = target_rsi_values\n",
    "\n",
    "initial_centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "982f7928",
   "metadata": {},
   "source": [
    "# 7. For each month select assets based on the cluster and form a portfolio based on Efficient Frontier max sharpe ratio optimization\n",
    "* First we will filter only stocks corresponding to the cluster we choose based on our hypothesis.\n",
    "\n",
    "* Momentum is persistent and my idea would be that stocks clustered around RSI 70 centroid should continue to outperform in the following month - thus I would select stocks corresponding to cluster 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e7c2b46",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "filtered_df = data[data['cluster']==3].copy()  # eveything lies here so use cluster value carefull\n",
    "\n",
    "filtered_df = filtered_df.reset_index(level=1)\n",
    "\n",
    "filtered_df.index = filtered_df.index+pd.DateOffset(1)\n",
    "\n",
    "filtered_df = filtered_df.reset_index().set_index(['date', 'ticker'])\n",
    "\n",
    "dates = filtered_df.index.get_level_values('date').unique().tolist()\n",
    "\n",
    "fixed_dates = {} # declare dictionary\n",
    "\n",
    "for d in dates:\n",
    "    \n",
    "    fixed_dates[d.strftime('%Y-%m-%d')] = filtered_df.xs(d, level=0).index.tolist()\n",
    "\n",
    "fixed_dates\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb319424",
   "metadata": {},
   "source": [
    "# Define portfolio optimization function\n",
    "* We will define a function which optimizes portfolio weights using PyPortfolioOpt package and EfficientFrontier optimizer to maximize the sharpe ratio.\n",
    "\n",
    "* To optimize the weights of a given portfolio we would need to supply last 1 year prices to the function.\n",
    "\n",
    "* Apply signle stock weight bounds constraint for diversification (minimum half of equaly weight and maximum 10% of portfolio)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "712acc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypfopt.efficient_frontier import EfficientFrontier\n",
    "from pypfopt import risk_models\n",
    "from pypfopt import expected_returns\n",
    "\n",
    "def optimize_weights(prices, lower_bound=0):\n",
    "    \n",
    "    returns = expected_returns.mean_historical_return(prices=prices,\n",
    "                                                      frequency=252)\n",
    "    \n",
    "    cov = risk_models.sample_cov(prices=prices,\n",
    "                                 frequency=252)\n",
    "    \n",
    "    ef = EfficientFrontier(expected_returns=returns,\n",
    "                           cov_matrix=cov,\n",
    "                           weight_bounds=(lower_bound, .1),\n",
    "                           solver='SCS')\n",
    "    \n",
    "    weights = ef.max_sharpe()\n",
    "    \n",
    "    return ef.clean_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "689e7f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "stocks = data.index.get_level_values('ticker').unique().tolist()\n",
    "new_df= yf.download(tickers= stocks,\n",
    "                    start=data.index.get_level_values('date').unique()[0]-pd.DateOffset(months=12)\n",
    "                    ,end=data.index.get_level_values('date').unique()[-1])\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc5bd6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.index.get_level_values('date').unique()[0]-pd.DateOffset(months=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca68e47",
   "metadata": {},
   "source": [
    "* Calculate daily returns for each stock which could land up in our portfolio.\n",
    "\n",
    "* Then loop over each month start, select the stocks for the month and calculate their weights for the next month.\n",
    "\n",
    "* If the maximum sharpe ratio optimization fails for a given month, apply equally-weighted weights.\n",
    "\n",
    "* Calculated each day portfolio return."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2db9e73a",
   "metadata": {},
   "outputs": [],
   "source": [
    "returns_dataframe = np.log(new_df['Adj Close']).diff()\n",
    "\n",
    "portfolio_df = pd.DataFrame()\n",
    "\n",
    "for start_date in fixed_dates.keys():\n",
    "    \n",
    "    try:\n",
    "\n",
    "        end_date = (pd.to_datetime(start_date)+pd.offsets.MonthEnd(0)).strftime('%Y-%m-%d')\n",
    "\n",
    "        cols = fixed_dates[start_date]\n",
    "\n",
    "        optimization_start_date = (pd.to_datetime(start_date)-pd.DateOffset(months=12)).strftime('%Y-%m-%d')\n",
    "\n",
    "        optimization_end_date = (pd.to_datetime(start_date)-pd.DateOffset(days=1)).strftime('%Y-%m-%d')\n",
    "        \n",
    "        optimization_df = new_df[optimization_start_date:optimization_end_date]['Adj Close'][cols]\n",
    "        \n",
    "        success = False\n",
    "        try:\n",
    "            weights = optimize_weights(prices=optimization_df,\n",
    "                                   lower_bound=round(1/(len(optimization_df.columns)*2),3))\n",
    "\n",
    "            weights = pd.DataFrame(weights, index=pd.Series(0))\n",
    "            \n",
    "            success = True\n",
    "        except:\n",
    "            print(f'Max Sharpe Optimization failed for {start_date}, Continuing with Equal-Weights')\n",
    "        \n",
    "        if success==False:\n",
    "            weights = pd.DataFrame([1/len(optimization_df.columns) for i in range(len(optimization_df.columns))],\n",
    "                                     index=optimization_df.columns.tolist(),\n",
    "                                     columns=pd.Series(0)).T\n",
    "        \n",
    "        temp_df = returns_dataframe[start_date:end_date]\n",
    "\n",
    "        temp_df = temp_df.stack().to_frame('return').reset_index(level=0)\\\n",
    "                   .merge(weights.stack().to_frame('weight').reset_index(level=0, drop=True),\n",
    "                          left_index=True,\n",
    "                          right_index=True)\\\n",
    "                   .reset_index().set_index(['Date', 'index']).unstack().stack()\n",
    "\n",
    "        temp_df.index.names = ['date', 'ticker']\n",
    "\n",
    "        temp_df['weighted_return'] = temp_df['return']*temp_df['weight']\n",
    "\n",
    "        temp_df = temp_df.groupby(level=0)['weighted_return'].sum().to_frame('Strategy Return')\n",
    "\n",
    "        portfolio_df = pd.concat([portfolio_df, temp_df], axis=0)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "portfolio_df = portfolio_df.drop_duplicates()\n",
    "\n",
    "portfolio_df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c61638e",
   "metadata": {},
   "outputs": [],
   "source": [
    "portfolio_df.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "954a9410",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame([1/len(optimization_df.columns) for i in range(len(optimization_df.columns))],\n",
    "                                     index=optimization_df.columns.tolist(),\n",
    "                                     columns=pd.Series(0)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9f38c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "nifty = yf.download(tickers='^NSEI',\n",
    "                  start='2015-01-01',\n",
    "                  end=dt.date.today())\n",
    "\n",
    "nifty_ret = np.log(nifty[['Adj Close']]).diff().dropna().rename({'Adj Close':'nifty Buy&Hold'}, axis=1)\n",
    "\n",
    "portfolio_df = portfolio_df.merge(nifty_ret,\n",
    "                                  left_index=True,\n",
    "                                  right_index=True)\n",
    "\n",
    "portfolio_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c291d74c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mtick\n",
    "unique_clusters = data['cluster'].unique()\n",
    "\n",
    "for cluster_number in unique_clusters:\n",
    "    filtered_df = data[data['cluster'] == cluster_number].copy()\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "portfolio_cumulative_return = np.exp(np.log1p(portfolio_df).cumsum())-1\n",
    "\n",
    "portfolio_cumulative_return[:'2023-09-29'].plot(figsize=(16,6))\n",
    "\n",
    "plt.title('Unsupervised Learning Trading Strategy Returns Over Time for cluster ')\n",
    "\n",
    "plt.gca().yaxis.set_major_formatter(mtick.PercentFormatter(1))\n",
    "\n",
    "plt.ylabel('Return')\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8f4ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming 'portfolio_df' has the 'Strategy Return' and 'nifty Buy&Hold' columns\n",
    "strategy_returns = portfolio_df['Strategy Return']\n",
    "nifty_returns = portfolio_df['nifty Buy&Hold']\n",
    "\n",
    "# Calculate Sharpe ratio for the strategy\n",
    "strategy_sharpe_ratio = np.sqrt(252) * strategy_returns.mean() / strategy_returns.std()\n",
    "\n",
    "# Calculate Sharpe ratio for NIFTY 50\n",
    "nifty_sharpe_ratio = np.sqrt(252) * nifty_returns.mean() / nifty_returns.std()\n",
    "\n",
    "# Print the Sharpe Ratios\n",
    "print(f\"Strategy Sharpe Ratio: {strategy_sharpe_ratio:.4f}\")\n",
    "print(f\"NIFTY 50 Sharpe Ratio: {nifty_sharpe_ratio:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41acbfda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928127ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Given data\n",
    "stock_data = {\n",
    "    'BAJFINANCE': [7259.35, 7314.55, 0.760399],\n",
    "    'ICICIBANK': [946.70, 1015.70, 7.28848],\n",
    "    'INDUSINDBK': [1461.15, 1497.50, 2.48777],\n",
    "    'ITC': [449.90, 453.10, 0.711269],\n",
    "    'KOTAKBANK': [1750.50, 1824.35, 4.21879],\n",
    "    'SBIN': [571.75, 612.40, 7.10975],\n",
    "    'ASIANPAINT': [3173.40, 3243.65, 2.21371],\n",
    "    'CIPLA': [1205.70, 1216.50, 0.895745],\n",
    "    'TCS': [3511.65, 3593.55, 2.33224],\n",
    "    'UPL': [575.15, 599.35, 4.2076]\n",
    "}\n",
    "\n",
    "# Calculating total increase\n",
    "total_increase = sum(stock[2] for stock in stock_data.values())\n",
    "\n",
    "# Displaying the total increase\n",
    "print(f'Total Increase: {total_increase:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6514d575",
   "metadata": {},
   "outputs": [],
   "source": [
    "apar= yf.download('APARINDS.NS',start=\"2010-01-01\",end=\"2024-02-01\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73ae3f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abeb743",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab7ae745",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ebd3974",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f04a85a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
